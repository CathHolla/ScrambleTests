{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import math\n",
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(s1, s2, labels):\n",
    "    f1 = open(s1)\n",
    "    f2 = open(s2)\n",
    "    lab = open(labels)\n",
    "    sent1 = []\n",
    "    for line in f1:\n",
    "        sent1.append(line.replace(\" \\n\", \"\").split(\" \"))\n",
    "    sent2 = []\n",
    "    for line in f2:\n",
    "        sent2.append(line.replace(\" \\n\", \"\").split(\" \"))\n",
    "    labs = []\n",
    "    for line in lab:\n",
    "        labs.append(line.replace(\"\\n\", \"\"))\n",
    "    f1.close()\n",
    "    f2.close()\n",
    "    lab.close()\n",
    "    return sent1, sent2, labs\n",
    "\n",
    "s1, s2, labels = read_data(\"s1.train\", \"s2.train\", \"labels.train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is Data Balanced?\n",
    "label_counts = {}\n",
    "for label in labels:\n",
    "    if not label in label_counts:\n",
    "        label_counts[label] = 0\n",
    "    label_counts[label] += 1\n",
    "print \"label_counts: \", label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "num_sent = len(s1)\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "import torch\n",
    "import pickle\n",
    "import regrFuncs as rF\n",
    "import testFuncs as tF\n",
    "import random\n",
    "\n",
    "GLOVE_PATH = './Downloads/glove.840B.300d.txt'\n",
    "MODEL_PATH = './Downloads/infersent.allnli.pickle'\n",
    "REGR_MODEL_PATH = './models/'\n",
    "EMBED_STORE = None\n",
    "TEST_OUT_PATH = './regout/'\n",
    "DATA_PATH = './Downloads/SNLI/true/'\n",
    "\n",
    "outpaths = {'REGR_MODEL_PATH': REGR_MODEL_PATH, 'TEST_OUT_PATH': TEST_OUT_PATH}\n",
    "\n",
    "\n",
    "id2label = {0:'CONTRADICTION', 1:'NEUTRAL', 2:'ENTAILMENT'}\n",
    "label2id = {'CONTRADICTION': 0, 'NEUTRAL':1, 'ENTAILMENT':2}\n",
    "\n",
    "print \"MODEL_PATH=\", MODEL_PATH\n",
    "print \"cwd=\", os.getcwd()\n",
    "\n",
    "model = torch.load(MODEL_PATH, map_location=lambda storage, loc: storage)\n",
    "model.use_cuda = False\n",
    "model.set_glove_path(GLOVE_PATH)\n",
    "model.build_vocab_k_words(K=100000)\n",
    "\n",
    "names = ['InferSent', 'BOW']\n",
    "classifiers = [ 'LogReg']\n",
    "all_regs = {}\n",
    "for name in names:\n",
    "    for classifier in classifiers:\n",
    "        all_regs[name+classifier] = pickle.load(open('{0}{1}'.format(outpaths['REGR_MODEL_PATH'], name+classifier), 'rb'))\n",
    "\n",
    "def print_preds(sent_a, sent_b, verbose = True, names = names, classifiers = classifiers):\n",
    "    vals = {}\n",
    "    for name in names:\n",
    "        for classifier in classifiers:\n",
    "            A, B = rF.embed(model, sent_a, 1, name), rF.embed(model, sent_b, 1, name)\n",
    "            pred, conf = tF.predict(A, B, all_regs[name+classifier])\n",
    "            if verbose:\n",
    "                print('*'*20)\n",
    "                print(name, classifier)\n",
    "                print('*'*20, '\\n')\n",
    "            vals[name + classifier] = {}\n",
    "            vals[name + classifier]['pred'] = []\n",
    "            vals[name + classifier]['conf'] = []\n",
    "            for i in range(len(A)):\n",
    "                if verbose:\n",
    "                    print('A: ', sent_a[i], '\\t B: ', sent_b[i])\n",
    "                    print(id2label[pred[i]], conf[i][pred[i]]*100)\n",
    "                    print('\\n')\n",
    "                vals[name + classifier]['pred'].append(id2label[pred[i]])\n",
    "                vals[name + classifier]['conf'].append(conf[i][pred[i]]*100)\n",
    "            vals[name + classifier]['pred'] = np.array(vals[name + classifier]['pred'])\n",
    "            vals[name + classifier]['conf'] = np.array(vals[name + classifier]['conf'])\n",
    "            if verbose:\n",
    "                print('\\n\\n')\n",
    "    return vals\n",
    "\n",
    "from sets import Set\n",
    "def overlap_ratio(sent1, sent2):\n",
    "    sent1 = [word.lower() for word in sent1]\n",
    "    sent2 = [word.lower() for word in sent2]\n",
    "    word1 = Set(sent1)\n",
    "    word2 = Set(sent2)\n",
    "    cnt = 0\n",
    "    for word in sent2:\n",
    "        if word in word1:\n",
    "            cnt += 1\n",
    "    for word in sent1:\n",
    "        if word in word2:\n",
    "            cnt += 1\n",
    "    return 1.0 * cnt / (len(sent1) + len(sent2))\n",
    "\n",
    "overlaps = []\n",
    "for i in range(num_sent):\n",
    "    overlaps.append((i, overlap_ratio(s1[i], s2[i])))\n",
    "\n",
    "overlaps = sorted(overlaps, key=lambda p:-p[1])\n",
    "overlap_indices = [p[0] for p in overlaps]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# why high overlap = relevance?\n",
    "def general_stats(indices, k=5):\n",
    "    counter = {\"entailment\": 0, \"contradiction\":0, \"neutral\":0}\n",
    "    c_indices = []\n",
    "    for ind in indices:\n",
    "        counter[labels[ind]] += 1\n",
    "        if labels[ind] == \"contradiction\":\n",
    "            c_indices.append(ind)\n",
    "    print \"Top %d:\" % len(indices)\n",
    "    \n",
    "    for l in counter:\n",
    "        c = counter[l]\n",
    "        print \"%s:%d (%.3lf)\" % (l, c, 1.0 * c / len(indices))\n",
    "    \n",
    "    for i in range(k):\n",
    "        ind = np.random.choice(indices)\n",
    "        print \"    SAMPLE\\n    s1=%s\\n    s2=%s\\n    label=%s\" % (\" \".join(s1[ind]), \" \".join(s2[ind]), labels[ind])\n",
    "general_stats(overlap_indices)\n",
    "general_stats(overlap_indices[:1000])\n",
    "general_stats(overlap_indices[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Contradiction Analysis\n",
    "def contradiction_analysis(indices, predicts=None):\n",
    "    true_c, predict_c, match_c = 0, 0, 0\n",
    "    \n",
    "    subset_s1 = [\" \".join(s1[ind]) for ind in indices]\n",
    "    subset_s2 = [\" \".join(s2[ind]) for ind in indices]\n",
    "    if predicts is None:\n",
    "        predicts = print_preds(subset_s1, subset_s2)\n",
    "    \n",
    "    for i, ind in enumerate(indices):\n",
    "        if labels[ind] == \"contradiction\":\n",
    "            true_c += 1\n",
    "        if predicts[\"InferSentLogReg\"][\"pred\"][i] == \"CONTRADICTION\":\n",
    "            predict_c += 1\n",
    "            if labels[ind] == \"contradiction\":\n",
    "                match_c += 1\n",
    "    print \"True contradictions: %d / %d (%.3lf)\" % (true_c, len(indices), 1.0 * true_c / len(indices))\n",
    "    print \"Predicted contradictions: %d / %d (%.3lf)\" % (predict_c, len(indices), 1.0 * predict_c / len(indices))\n",
    "    print \"How many true contradictions are predicted correctly? %d / %d ( %.3lf)\" % (match_c, true_c, 1.0 * match_c / true_c)\n",
    "    print \"How many predicted contradictions are true? %d / %d (%.3lf)\" % (match_c, predict_c, 1.0 * match_c / predict_c)\n",
    "    return predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = contradiction_analysis(overlap_indices[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "def sample_data_point(indices, label):\n",
    "    # not entirely random at the moment\n",
    "    ind = np.random.choice(indices)\n",
    "    while (labels[ind] != label):\n",
    "        ind = np.random.choice(indices)\n",
    "    return \"[%d]%s\\n%s\\n%s\\n\" % (ind, labels[ind], \" \".join(s1[ind]), \" \".join(s2[ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print sample_data_point(overlap_indices[:1000], \"contradiction\")\n",
    "print sample_data_point(overlap_indices[:1000], \"contradiction\")\n",
    "\n",
    "print sample_data_point(overlap_indices[:1000], \"entailment\")\n",
    "print sample_data_point(overlap_indices[:1000], \"entailment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_data_point_v2(indices, predicts, k=10):\n",
    "    selected_indices = []\n",
    "    for i, ind in enumerate(indices):\n",
    "        if labels[ind] == \"contradiction\" and predicts[\"InferSentLogReg\"][\"pred\"][i] == \"CONTRADICTION\":\n",
    "            selected_indices.append(i)\n",
    "    for t in range(k):\n",
    "        i = np.random.choice(selected_indices)\n",
    "        ind = indices[i]\n",
    "        print \"P(C_pred | C_true) SAMPLE %d:\" % t\n",
    "        print \"    %s\\n    %s\\npredict=%s\\n\" % (\" \".join(s1[ind]), \" \".join(s2[ind]), predicts[\"InferSentLogReg\"][\"pred\"][i])\n",
    "    \n",
    "    selected_indices = []\n",
    "    for i, ind in enumerate(indices):\n",
    "        if labels[ind] == \"contradiction\" and predicts[\"InferSentLogReg\"][\"pred\"][i] != \"CONTRADICTION\":\n",
    "            selected_indices.append(i)\n",
    "    for t in range(k):\n",
    "        i = np.random.choice(selected_indices)\n",
    "        ind = indices[i]\n",
    "        print \"P(not C_pred | C_true) SAMPLE %d:\" % t\n",
    "        print \"    %s\\n    %s\\npredict=%s\\n\" % (\" \".join(s1[ind]), \" \".join(s2[ind]), predicts[\"InferSentLogReg\"][\"pred\"][i])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_point_v2(overlap_indices[:1000], predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negation Analysis\n",
    "negation_words_basic = [\"no\", \"not\", \"didn't\", \"don't\", \"doesn't\", \"n't\", ]\n",
    "negation_words_advanced = negation_words_basic + [\"neither\", \"none\", \"never\", \"nobody\", \"nothing\", \"nowhere\", \"neither\", \"little\", \"hardly\", \"few\", \"rarely\"]\n",
    "\"\"\" check if there's how many negation sentence is contradiction\"\"\"\n",
    "from sets import Set\n",
    "\n",
    "def has_negation(sent1, sent2, negation_words):\n",
    "    sent = Set(sent1) | Set(sent2)\n",
    "    return len(Set(negation_words) & sent) != 0\n",
    "    \n",
    "def negation_words_analysis(indices, negation_words, label=\"contradiction\"):\n",
    "    # negation / contradictions\n",
    "    \n",
    "    \n",
    "    total = 0\n",
    "    counter = 0\n",
    "    for ind in indices:\n",
    "        if \" \".join(s1[ind]) == \" \".join(s2[ind]):\n",
    "            continue\n",
    "        if labels[ind] == label:\n",
    "            total += 1\n",
    "            if has_negation(s1[ind], s2[ind], negation_words):\n",
    "                counter += 1\n",
    "            #else:\n",
    "                #print \"s1=\", s1[ind]\n",
    "                #print \"s2=\", s2[ind]\n",
    "                #print \"labels=\", labels[ind]\n",
    "                #print \"\\n\\n\"\n",
    "    print \"negation/%s: %d/%d (%.3lf) %s pairs have negations\" % (label, counter, total, 1.0*counter/total, label)\n",
    "    # contradictions / negations\n",
    "    total, counter = 0, 0\n",
    "    for ind in indices:\n",
    "        if \" \".join(s1[ind]) == \" \".join(s2[ind]):\n",
    "            continue\n",
    "\n",
    "        if has_negation(s1[ind], s2[ind], negation_words):\n",
    "            total += 1\n",
    "            if labels[ind] == label:\n",
    "                counter += 1\n",
    "    print \"%s/negation: %d/%d (%.3lf) negation pairs are %s\" % (label, counter, total, 1.0*counter/total, label)\n",
    "\n",
    "print \"high overlap/relevancy top 10000\"\n",
    "negation_words_analysis(overlap_indices[:10000], negation_words_advanced)\n",
    "negation_words_analysis(overlap_indices[:10000], negation_words_advanced, label=\"entailment\")\n",
    "\n",
    "print \"overall\"\n",
    "negation_words_analysis(overlap_indices, negation_words_advanced)\n",
    "negation_words_analysis(overlap_indices, negation_words_advanced, label=\"entailment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def negation_words_analysis_infersent(indices, negation_words, label=\"CONTRADICTION\", predicts=None):\n",
    "    # negation / contradictions\n",
    "    if predicts == None:\n",
    "        subset_s1 = [\" \".join(s1[ind]) for ind in indices]\n",
    "        subset_s2 = [\" \".join(s2[ind]) for ind in indices]\n",
    "        predicts = print_preds(subset_s1, subset_s2)\n",
    "\n",
    "    total = 1\n",
    "    counter = 0\n",
    "    for i, ind in enumerate(indices):\n",
    "        if \" \".join(s1[ind]) == \" \".join(s2[ind]):\n",
    "            continue\n",
    "        if predicts[\"InferSentLogReg\"][\"pred\"][i] == label:\n",
    "            total += 1\n",
    "            if has_negation(s1[ind], s2[ind], negation_words):\n",
    "                counter += 1\n",
    "            #else:\n",
    "                #print \"s1=\", s1[ind]\n",
    "                #print \"s2=\", s2[ind]\n",
    "                #print \"labels=\", labels[ind]\n",
    "                #print \"\\n\\n\"\n",
    "    print \"negation/%s: %d/%d (%.3lf) %s pairs have negations\" % (label, counter, total, 1.0*counter/total, label)\n",
    "    # contradictions / negations\n",
    "    total, counter = 1, 0\n",
    "    for i, ind in enumerate(indices):\n",
    "        if \" \".join(s1[ind]) == \" \".join(s2[ind]):\n",
    "            continue\n",
    "\n",
    "        if has_negation(s1[ind], s2[ind], negation_words):\n",
    "            print \"HAS NEGATION !!!!!!!!!!!!!!!!!!!!!!!!! \\n%s\\n%s\\npredict=%s, true=%s\\n\" % (s1[ind], s2[ind], predicts[\"InferSentLogReg\"][\"pred\"][i], labels[ind])\n",
    "            total += 1\n",
    "            if predicts[\"InferSentLogReg\"][\"pred\"][i] == label:\n",
    "                counter += 1\n",
    "    print \"%s/negation: %d/%d (%.3lf) negation pairs are %s\" % (label, counter, total, 1.0*counter/total, label)\n",
    "    \n",
    "    return predicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"high overlap/relevancy top 1000\"\n",
    "\n",
    "predicts = negation_words_analysis_infersent(overlap_indices[:1000], negation_words_basic)\n",
    "_ = negation_words_analysis_infersent(overlap_indices[:1000], negation_words_basic, \"ENTAILMENT\", predicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = negation_words_analysis_infersent(overlap_indices[:1000], negation_words_basic, predicts=predicts)\n",
    "_ = negation_words_analysis_infersent(overlap_indices[:1000], negation_words_basic, label=\"ENTAILMENT\", predicts=predicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"How many contradictory pair of sentences has antonyms + print out importance of those antonyms\"\"\"\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from sets import Set\n",
    "from tqdm import tqdm \n",
    "def get_antonym_set(word):\n",
    "    antonyms = Set([])\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            if l.antonyms():\n",
    "                antonyms.add(l.antonyms()[0].name())\n",
    "    return antonyms\n",
    "def antonym_analysis(indices, label=\"contradiction\", imp_sample=10):\n",
    "    sum_antonym = 0\n",
    "    counter = 0\n",
    "    total = 0\n",
    "    total_ant = 0\n",
    "    for ind in tqdm(indices):\n",
    "        if labels[ind] == label:\n",
    "            total += 1\n",
    "        cur_antonym = 0\n",
    "        \n",
    "        \n",
    "        for word in s1[ind]:\n",
    "            antonym = get_antonym_set(word)\n",
    "            if len(Set(s2[ind]) & antonym) != 0:\n",
    "                cur_antonym += 1\n",
    "                \n",
    "                if labels[ind] == label:\n",
    "                    print \"ind=%d\\ns1:%s\\ns2:%s\\nAntonym: %s, %s\" % (ind, \" \".join(s1[ind]), \" \".join(s2[ind]), word, Set(s2[ind]) & antonym)\n",
    "        if labels[ind] == label:\n",
    "            counter += int(cur_antonym > 0)\n",
    "            sum_antonym += cur_antonym\n",
    "            \n",
    "        total_ant += int(cur_antonym > 0)\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        if imp_sample != 0 and cur_antonym > 0:\n",
    "            print \"label=\", labels[ind]\n",
    "            model.visualize(\" \".join(s1[ind]), tokenize=True)\n",
    "            model.visualize(\" \".join(s2[ind]), tokenize=True)\n",
    "            _, _, y1 = external_visualize(model,\" \".join(s1[ind]), tokenize=True,output_y=True)\n",
    "            _, _, y2 = external_visualize(model,\" \".join(s2[ind]), tokenize=True,output_y=True)\n",
    "            imp = 0\n",
    "            for idx, word in enumerate(s1[ind]):\n",
    "                antonym = get_antonym_set(word)\n",
    "                if len(Set(s2[ind]) & antonym) != 0:\n",
    "                    imp += y1[idx]\n",
    "                    for word2 in s2[ind]:\n",
    "                        if word2 in antonym:\n",
    "                            print \"find (%s, %s)\" % (word, word2)\n",
    "            imp /= cur_antonym\n",
    "            print \"avg imp of antonyms (in s1): %.3lf\" % (imp)\n",
    "            imp_sample -= 1\n",
    "        \"\"\"\n",
    "        \n",
    "    print \"avg antonym # for given type sentence: %.5lf\" % (sum_antonym * 1.0 / total)\n",
    "    print \"antonym/%s: percantage of given type sentence to have antonyms: %d/%d (%.5lf)\" % (label, counter, total, 1.0 * counter / total)\n",
    "    print \"%s/antonym: percentage of antonyms pairs sentences to be %s: %d/%d(%.5lf)\" % (label, label, counter, total_ant, 1.0 * counter / total_ant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antonym_analysis(overlap_indices[:1000])\n",
    "#antonym_analysis(overlap_indices[:1000], label=\"entailment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "antonym_analysis(overlap_indices)\n",
    "antonym_analysis(overlap_indices, label=\"entailment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def antonym_analysis_infersent(indices, predicts, label=\"CONTRADICTION\", imp_sample=10):\n",
    "    sum_antonym = 0\n",
    "    counter = 0\n",
    "    total = 0\n",
    "    total_ant = 0\n",
    "    for i, ind in tqdm(enumerate(indices)):\n",
    "        if predicts[\"InferSentLogReg\"][\"pred\"][i] == label:\n",
    "            total += 1\n",
    "        cur_antonym = 0\n",
    "        \n",
    "        \n",
    "        for word in s1[ind]:\n",
    "            antonym = get_antonym_set(word)\n",
    "            if len(Set(s2[ind]) & antonym) != 0:\n",
    "                cur_antonym += 1\n",
    "                \n",
    "                if predicts[\"InferSentLogReg\"][\"pred\"][i] == label:\n",
    "                    print \"ind=%d\\ns1:%s\\ns2:%s\\nAntonym: %s, %s\" % (ind, \" \".join(s1[ind]), \" \".join(s2[ind]), word, Set(s2[ind]) & antonym)\n",
    "        if predicts[\"InferSentLogReg\"][\"pred\"][i] == label:\n",
    "            counter += int(cur_antonym > 0)\n",
    "            sum_antonym += cur_antonym\n",
    "            \n",
    "        total_ant += int(cur_antonym > 0)\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        if imp_sample != 0 and cur_antonym > 0:\n",
    "            print \"label=\", labels[ind]\n",
    "            model.visualize(\" \".join(s1[ind]), tokenize=True)\n",
    "            model.visualize(\" \".join(s2[ind]), tokenize=True)\n",
    "            _, _, y1 = external_visualize(model,\" \".join(s1[ind]), tokenize=True,output_y=True)\n",
    "            _, _, y2 = external_visualize(model,\" \".join(s2[ind]), tokenize=True,output_y=True)\n",
    "            imp = 0\n",
    "            for idx, word in enumerate(s1[ind]):\n",
    "                antonym = get_antonym_set(word)\n",
    "                if len(Set(s2[ind]) & antonym) != 0:\n",
    "                    imp += y1[idx]\n",
    "                    for word2 in s2[ind]:\n",
    "                        if word2 in antonym:\n",
    "                            print \"find (%s, %s)\" % (word, word2)\n",
    "            imp /= cur_antonym\n",
    "            print \"avg imp of antonyms (in s1): %.3lf\" % (imp)\n",
    "            imp_sample -= 1\n",
    "        \"\"\"\n",
    "    print \"avg antonym # for given type sentence: %.5lf\" % (sum_antonym * 1.0 / total)\n",
    "    print \"antonym/%s: percantage of given type sentence to have antonyms: %d/%d (%.5lf)\" % (label, counter, total, 1.0 * counter / total)\n",
    "    print \"%s/antonym: percentage of antonyms pairs sentences to be %s: %d/%d(%.5lf)\" % (label, label, counter, total_ant, 1.0 * counter / total_ant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antonym_analysis_infersent(overlap_indices[:1000], predicts)\n",
    "antonym_analysis_infersent(overlap_indices[:1000], predicts, label=\"ENTAILMENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
